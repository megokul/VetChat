{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b40a09d-0265-429a-be02-37056972dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c75745-1e19-4127-be7e-5110b4791892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_to_dataframes(directory=\".\"):\n",
    "    \"\"\"\n",
    "    Load all .jsonl files in the specified directory and its subdirectories into DataFrames,\n",
    "    storing them in a dictionary with filenames as keys.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The directory to search for .jsonl files. Defaults to the current directory.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where each key is the filename and the value is the corresponding DataFrame.\n",
    "    \"\"\"\n",
    "    data_frames = {}\n",
    "    # Search for all .jsonl files in the directory and subdirectories\n",
    "    for file_path in glob.glob(os.path.join(directory, '**/*.jsonl'), recursive=True):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                data.append(json.loads(line))\n",
    "        # Create a DataFrame from list of dictionaries\n",
    "        df = pd.DataFrame(data)\n",
    "        # Extract filename without extension as the key\n",
    "        filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        data_frames[filename] = df\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95b30ed-0cc5-40eb-a453-ebcb6b295173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "dataframes = load_jsonl_to_dataframes()  # Use the function for the current directory\n",
    "file_keys = dataframes.keys()\n",
    "file_key_list = list(file_keys)\n",
    "exp_name = file_key_list[0]\n",
    "df_output = dataframes[exp_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28800da-c346-412e-b683-51df395266f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symptom_string(main_string):\n",
    "\n",
    "    main_string = str(main_string)\n",
    "    \n",
    "    # Regular expression to find the dictionaries in the string\n",
    "    pattern1 = \"\\\\{'.*':\\\\s*'\\\\w*',\\\\s\\\\s'shown_signs':.*\\\\s*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\s.*\\\\}\\\\}\"\n",
    "    pattern2 = \"\\\\{.*\\\\}\"\n",
    "    \n",
    "    # Find all matches in the main string\n",
    "    match1 = re.findall(pattern1, main_string)\n",
    "    # Find all matches in the main string\n",
    "    match2 = re.findall(pattern2, main_string)\n",
    "\n",
    "    final_matches = match1 + match2\n",
    "    \n",
    "    return final_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77ab0f5-f317-4fe0-91ec-10b9eab10988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_animal_string(main_string):\n",
    "\n",
    "    main_string = str(main_string)\n",
    "\n",
    "    pattern = r\"animal:\\s*(\\w+)\"\n",
    "\n",
    "    match = re.findall(pattern, main_string)\n",
    "    \n",
    "    return match\n",
    "\n",
    "def check_exact_match(test_answer, response):\n",
    "    \n",
    "    # Compare dictionaries\n",
    "    exact_match = test_answer == response\n",
    "\n",
    "    return exact_match\n",
    "\n",
    "def check_animal_match(test_answer, response):\n",
    "\n",
    "    test_answer_extract = find_symptom_string(test_answer)\n",
    "    test_answer_dict = ast.literal_eval(test_answer_extract[0])\n",
    "    test_animal = test_answer_dict['animal']\n",
    "\n",
    "    response_animal = find_animal_string(response)\n",
    "    \n",
    "    # Compare the animal fields\n",
    "    animal_match = response_animal == response_animal\n",
    "\n",
    "    return animal_match\n",
    "\n",
    "def get_first_dict(string_data):\n",
    "    \n",
    "    start_idx = string_data.find(\"{\")\n",
    "    end_idx = string_data.find(\"}\", start_idx) + 1\n",
    "    first_dict_str = string_data[start_idx:end_idx]\n",
    "    \n",
    "    # Count the number of '{' and '}' in first_dict_str\n",
    "    open_braces_count = first_dict_str.count(\"{\")\n",
    "    close_braces_count = first_dict_str.count(\"}\")\n",
    "    \n",
    "    # If the number of '{' is more than '}', add '}' at the end of first_dict_str\n",
    "    while open_braces_count > close_braces_count:\n",
    "        first_dict_str += \"}\"\n",
    "        close_braces_count += 1\n",
    "    \n",
    "    return first_dict_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed53acf6-6b7e-46bd-b000-69791957041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shown_signs_accuracy(test_answer, response):\n",
    "    \n",
    "    test_answer_extract = find_symptom_string(test_answer)\n",
    "    test_answer_dict = ast.literal_eval(test_answer_extract[0])\n",
    "\n",
    "    response = get_first_dict(response)\n",
    "    response_symptoms = find_symptom_string(response)\n",
    "\n",
    "    # Check if response_symptoms is None or an empty list\n",
    "    if not response_symptoms:\n",
    "        return 0  # Return 0% accuracy if there are no symptoms in the response\n",
    "    \n",
    "    test_signs = test_answer_dict.get('shown_signs', {})\n",
    "    \n",
    "    response_signs = ast.literal_eval(response_symptoms[0])\n",
    "    \n",
    "    # Calculate the accuracy of shown_signs\n",
    "    correct_matches = 0\n",
    "    total_signs = len(test_signs)\n",
    "    \n",
    "    for sign, value in test_signs.items():\n",
    "        if sign in response_signs and response_signs[sign] == value:\n",
    "            correct_matches += 1\n",
    "    \n",
    "    # Calculate accuracy as a percentage\n",
    "    accuracy = (correct_matches / total_signs) * 100 if total_signs > 0 else 0\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ba53c9-3e6c-4576-bad6-7f223291236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shown_signs_metrics(test_answer, response):\n",
    "    \n",
    "    test_answer_extract = find_symptom_string(test_answer)\n",
    "    test_answer_dict = ast.literal_eval(test_answer_extract[0])\n",
    "    response_symptoms = find_symptom_string(response)\n",
    "    \n",
    "    # Extract shown_signs from both dictionaries\n",
    "    test_signs = test_answer_dict.get('shown_signs', {})\n",
    "    \n",
    "    # Check if response_symptoms is None or an empty list\n",
    "    if not response_symptoms:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    response_symptoms_first = get_first_dict(response_symptoms[0])\n",
    "\n",
    "    # Handle case where response_symptoms is an empty list\n",
    "    if response_symptoms:\n",
    "        response_signs = ast.literal_eval(response_symptoms_first)\n",
    "    else:\n",
    "        response_signs = set()\n",
    "    \n",
    "    # Get all unique signs\n",
    "    all_signs = set(test_signs.keys()).union(set(response_signs.keys()))\n",
    "    \n",
    "    # Create lists for true values and predicted values\n",
    "    y_true = [test_signs.get(sign, 0) for sign in all_signs]\n",
    "    y_pred = [response_signs.get(sign, 0) for sign in all_signs]\n",
    "    y_pred = [x if x in (-1, 0, 1) else 3 for x in y_pred]\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f8797f-9df3-4075-b2e2-109aa1772763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_extra_signs(test_answer, response):\n",
    "    \n",
    "    test_answer_extract = find_symptom_string(test_answer)\n",
    "    test_answer_dict = ast.literal_eval(test_answer_extract[0])\n",
    "\n",
    "    response = get_first_dict(response)\n",
    "    response_symptoms = find_symptom_string(response)\n",
    "    \n",
    "    # Extract shown_signs from both dictionaries\n",
    "    test_signs = set(test_answer_dict.get('shown_signs', {}))\n",
    "    \n",
    "    # Handle case where response_symptoms is an empty list\n",
    "    if response_symptoms:\n",
    "        response_signs = set(ast.literal_eval(response_symptoms[0]))\n",
    "    else:\n",
    "        response_signs = set()\n",
    "    \n",
    "    # Identify missing and extra signs\n",
    "    missing_signs = test_signs - response_signs\n",
    "    extra_signs = response_signs - test_signs\n",
    "    \n",
    "    return missing_signs, extra_signs, len(missing_signs), len(extra_signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f0defe-ca26-4559-886a-284a319abb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_result_for_df(dataframe, exp_name):\n",
    "    # Initialize lists to store the results\n",
    "    exact_match_results = []\n",
    "    animal_match_results = []\n",
    "    shown_signs_accuracy_results = []\n",
    "    shown_signs_precision_results = []\n",
    "    shown_signs_recall_results = []\n",
    "    shown_signs_f1_score_results = []\n",
    "    missing_signs_results = []\n",
    "    extra_signs_results = []\n",
    "    num_missing_signs_results = []\n",
    "    num_extra_signs_results = []\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for i in range(len(dataframe)):\n",
    "        \n",
    "        test_answer = dataframe['test_answer'][i]\n",
    "        response = dataframe['response'][i]\n",
    "\n",
    "        exact_match = check_exact_match(test_answer, response)\n",
    "        animal_match = check_animal_match(test_answer, response)\n",
    "        shown_signs_accuracy = check_shown_signs_accuracy(test_answer, response)\n",
    "        precision, recall, f1 = check_shown_signs_metrics(test_answer, response)\n",
    "        missing_signs, extra_signs, num_missing_signs, num_extra_signs = check_missing_extra_signs(test_answer, response)\n",
    "\n",
    "        # Append the results to the lists\n",
    "        exact_match_results.append(exact_match)\n",
    "        animal_match_results.append(animal_match)\n",
    "        shown_signs_accuracy_results.append(shown_signs_accuracy)\n",
    "        shown_signs_precision_results.append(precision)\n",
    "        shown_signs_recall_results.append(recall)\n",
    "        shown_signs_f1_score_results.append(f1)\n",
    "        missing_signs_results.append(list(missing_signs))\n",
    "        extra_signs_results.append(list(extra_signs))\n",
    "        num_missing_signs_results.append(num_missing_signs)\n",
    "        num_extra_signs_results.append(num_extra_signs)\n",
    "\n",
    "    # Create a new dataframe with the original data and the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'exp_name': exp_name,\n",
    "        'exact_match': exact_match_results,\n",
    "        'animal_match': animal_match_results,\n",
    "        'shown_signs_accuracy': shown_signs_accuracy_results,\n",
    "        'shown_signs_precision': shown_signs_precision_results,\n",
    "        'shown_signs_recall': shown_signs_recall_results,\n",
    "        'shown_signs_f1_score': shown_signs_f1_score_results,\n",
    "        'missing_signs': missing_signs_results,\n",
    "        'extra_signs': extra_signs_results,\n",
    "        'num_missing_signs': num_missing_signs_results,\n",
    "        'num_extra_signs': num_extra_signs_results\n",
    "    })\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41bd163-9fbe-4c62-9aaf-f59595e2b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics(result_df):\n",
    "    \"\"\"\n",
    "    Calculate the average metrics for a given result dataframe.\n",
    "    \n",
    "    Args:\n",
    "    result_df (pd.DataFrame): The result dataframe containing metrics.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the average metrics.\n",
    "    \"\"\"\n",
    "    average_metrics = {\n",
    "        'exact_match': result_df['exact_match'].mean(),\n",
    "        'animal_match': result_df['animal_match'].mean(),\n",
    "        'shown_signs_accuracy': result_df['shown_signs_accuracy'].mean(),\n",
    "        'shown_signs_precision': result_df['shown_signs_precision'].mean(),\n",
    "        'shown_signs_recall': result_df['shown_signs_recall'].mean(),\n",
    "        'shown_signs_f1_score': result_df['shown_signs_f1_score'].mean(),\n",
    "        'num_missing_signs': result_df['num_missing_signs'].mean(),\n",
    "        'num_extra_signs': result_df['num_extra_signs'].mean()\n",
    "    }\n",
    "    \n",
    "    average_metrics_df = pd.DataFrame(average_metrics, index=[result_df['exp_name'].iloc[0]])\n",
    "    \n",
    "    return average_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab60647-edf2-4300-9382-0fc709d29840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_save_compiled_results(avg_result_df, filename='compiled_results.csv'):\n",
    "    \"\"\"\n",
    "    Update the compiled results DataFrame with new results and save it locally.\n",
    "    If the file already exists, it updates the DataFrame; otherwise, it creates a new one.\n",
    "\n",
    "    Args:\n",
    "    result_df (pd.DataFrame): The result DataFrame containing the new results.\n",
    "    filename (str): The filename for the compiled results CSV file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        compiled_results_df = pd.read_csv(filename)\n",
    "    else:\n",
    "        # Create an empty DataFrame with the specified columns\n",
    "        compiled_results_df = pd.DataFrame(columns=[\n",
    "            'exp_num', 'epoch_num', 'train/test', 'exact_match', 'animal_match',\n",
    "            'shown_signs_accuracy', 'shown_signs_precision', 'shown_signs_recall',\n",
    "            'shown_signs_f1_score', 'num_missing_signs', 'num_extra_signs'\n",
    "        ])\n",
    "    \n",
    "    # Extract expnum, epochnum, and train/test from the exp_name\n",
    "    exp_name = avg_result_df.index[0]\n",
    "    parts = exp_name.split('_')\n",
    "    epoch_num = parts[-1].replace('ep', '')\n",
    "    exp_num = parts[-2].replace('exp', '')\n",
    "    train_test = parts[-4]\n",
    "    model_name = parts[0]\n",
    "    \n",
    "    # Create a dictionary with the new data\n",
    "    new_data = {\n",
    "        'exp_num': exp_num,\n",
    "        'epoch_num': epoch_num,\n",
    "        'train/test': train_test,\n",
    "        'model': model_name,\n",
    "        'exact_match': round(avg_result_df.at[exp_name, 'exact_match'], 2),\n",
    "        'animal_match': round(avg_result_df.at[exp_name, 'animal_match'], 2),\n",
    "        'shown_signs_accuracy': round(avg_result_df.at[exp_name, 'shown_signs_accuracy'], 2),\n",
    "        'shown_signs_precision': round(avg_result_df.at[exp_name, 'shown_signs_precision'], 2),\n",
    "        'shown_signs_recall': round(avg_result_df.at[exp_name, 'shown_signs_recall'], 2),\n",
    "        'shown_signs_f1_score': round(avg_result_df.at[exp_name, 'shown_signs_f1_score'], 2),\n",
    "        'num_missing_signs': round(avg_result_df.at[exp_name, 'num_missing_signs'], 2),\n",
    "        'num_extra_signs': round(avg_result_df.at[exp_name, 'num_extra_signs'], 2)\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    new_df = pd.DataFrame(new_data, index=[exp_name])\n",
    "    \n",
    "    # Update the compiled results DataFrame\n",
    "    compiled_results_df = pd.concat([compiled_results_df, new_df], ignore_index=True)\n",
    "\n",
    "    compiled_results_df = compiled_results_df.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # Save the updated compiled results DataFrame to CSV\n",
    "    compiled_results_df.to_csv(filename, index=False)\n",
    "\n",
    "    return compiled_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721bf92-9817-4323-a8c2-8570ac639c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99b89167-4794-49c2-bb38-10ca026f6c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamgo\\AppData\\Local\\Temp\\ipykernel_33396\\3338035304.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  compiled_results_df = pd.concat([compiled_results_df, new_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results added for : pythia_test_results_exp0_ep10\n",
      "Results added for : pythia_test_results_exp0_ep20\n",
      "Results added for : pythia_test_results_exp0_ep3\n",
      "Results added for : pythia_test_results_exp1_ep10\n",
      "Results added for : pythia_test_results_exp1_ep20\n",
      "Results added for : pythia_test_results_exp1_ep3\n",
      "Results added for : pythia_test_results_exp2_ep10\n",
      "Results added for : pythia_test_results_exp2_ep20\n",
      "Results added for : pythia_test_results_exp2_ep3\n",
      "Results added for : pythia_test_results_exp3_ep10\n",
      "Results added for : pythia_test_results_exp3_ep3\n",
      "Results added for : pythia_test_results_exp4_ep20\n",
      "Results added for : pythia_test_results_exp4_ep3\n",
      "Results added for : pythia_test_results_exp5_ep10\n",
      "Results added for : pythia_test_results_exp5_ep20\n",
      "Results added for : pythia_test_results_exp5_ep3\n",
      "Results added for : pythia_train_results_exp0_ep10\n",
      "Results added for : pythia_train_results_exp0_ep20\n",
      "Results added for : pythia_train_results_exp0_ep3\n",
      "Results added for : pythia_train_results_exp1_ep10\n",
      "Results added for : pythia_train_results_exp1_ep20\n",
      "Results added for : pythia_train_results_exp1_ep3\n",
      "Results added for : pythia_train_results_exp2_ep10\n",
      "Results added for : pythia_train_results_exp2_ep20\n",
      "Results added for : pythia_train_results_exp2_ep3\n",
      "Results added for : pythia_train_results_exp3_ep10\n",
      "Results added for : pythia_train_results_exp3_ep3\n",
      "Results added for : pythia_train_results_exp4_ep20\n",
      "Results added for : pythia_train_results_exp4_ep3\n",
      "Results added for : pythia_train_results_exp5_ep10\n",
      "Results added for : pythia_train_results_exp5_ep20\n",
      "Results added for : pythia_train_results_exp5_ep3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(file_key_list)):\n",
    "    try:\n",
    "        exp_name = file_key_list[i]\n",
    "        df_output = dataframes[exp_name]\n",
    "        result_df = check_result_for_df(df_output, exp_name)\n",
    "        avg_result_df = calculate_average_metrics(result_df)\n",
    "        compiled_results_df = update_and_save_compiled_results(avg_result_df)\n",
    "        print(f'Results added for : {exp_name}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to process {exp_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f22336-c6fc-4226-89db-57f5028ad888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cdb3d-2bfe-47f8-9ab2-cca7c54816eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ae45c-b381-43de-b5a5-02684b47ae7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcb93d-b823-42c8-a3fb-bb97efff8cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
